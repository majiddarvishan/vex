#pragma once

#include <vex/networking/common.hpp>
#include <vex/networking/net/detail/ring_buffer.hpp>
#include <vex/networking/pdu.hpp>

#include <boost/asio.hpp>
#include <boost/asio/bind_executor.hpp>
#include <boost/asio/strand.hpp>

#include <algorithm>
#include <array>
#include <atomic>
#include <cstring>
#include <iostream>
#include <memory>
#include <optional>
#include <deque>
#include <vector>
#include <functional>
#include <span>
#include <cstdint>
#include <cassert>

#if defined(__linux__)
    #include <netinet/tcp.h>
    #include <sys/socket.h>
#endif

namespace pa::pinex
{
        static constexpr size_t header_length{10};
    static constexpr uint32_t max_command_length{10 * 1024 * 1024}; // 10MB
    static constexpr std::size_t default_ring_capacity = 1024 * 1024; // 1MB
    static constexpr std::size_t default_send_chunk = 64 * 1024;      // 64KB per async_write

struct Options {
    bool no_delay = true;
    int sndbuf = 262144;
    int rcvbuf = 262144;
    bool keepalive = true;
    size_t send_ring_capacity = default_ring_capacity;
    size_t max_send_chunk = default_send_chunk;
    size_t send_buf_threshold = 1024 * 1024; // when to mark buffer as above threshold
    size_t initial_reserve = 8192;
};

static void enable_keepalive(boost::asio::ip::tcp::socket& socket, uint16_t inactivity_timeout)
{
    boost::system::error_code ec;
    socket.set_option(boost::asio::socket_base::keep_alive(true), ec);
    if (ec) {
        std::cerr << "Warning: Failed to enable SO_KEEPALIVE: " << ec.message() << std::endl;
    }
    // Platform-specific tuning like TCP_KEEPIDLE, TCP_KEEPINTVL can be set
    // through native_handle + setsockopt if desired by your platform code.
}

using request = std::variant<std::monostate, bind_request, stream_request>;
using response = std::variant<std::monostate, bind_response, stream_response>;


/*
  Optimized session:
  - preserves public handler names from original header
  - uses a send ring (zero-copy) + fallback deque of chunks to avoid big reallocations
  - uses a small preallocated array of const_buffer to avoid per-send vector allocations
  - avoids shrink_to_fit(); uses swap-with-small-vector technique when trimming needed
  - provides two-span zero-copy parsing via pdu handler usage
*/

class session : public std::enable_shared_from_this<session>
{
  public:
    // user-provided handlers (kept from original API)
    std::function<void(std::shared_ptr<session>, std::optional<std::string>)> close_handler;
    std::function<void(request&&, uint32_t)> request_handler;
    std::function<void(response&&, uint32_t, command_status)> response_handler;
    std::function<void()> send_buf_available_handler;
    std::function<void(const std::string&, command_id, std::span<const uint8_t>)> deserialization_error_handler;

  private:
    enum class state { open, unbinding, close };
    enum class receiving_state { receiving, pending_pause, paused };

  public:
    explicit session(boost::asio::io_context& ioc, Options opts = Options{})
        : socket_(ioc),
          strand_(boost::asio::make_strand(ioc)),
          options_(opts),
          send_ring_(opts.send_ring_capacity),
          recv_ring_(opts.send_ring_capacity),
          state_(state::open),
          receiving_state_(receiving_state::receiving),
          send_in_progress_(false),
          send_buf_threshold_(opts.send_buf_threshold),
          stats_send_bytes_(0),
          stats_recv_bytes_(0)
    {
        // pre-reserve small vectors used in fallback to avoid immediate reallocations
        // Note: we intentionally do not shrink_to_fit() anywhere on hot path.
        writing_send_buf_.reserve(opts.initial_reserve);
        body_reuse_buf_.reserve(opts.initial_reserve);
    }

    session(const session&) = delete;
    session& operator=(const session&) = delete;

    ~session() {
        // ensure socket closed
        boost::system::error_code ec;
        socket_.close(ec);
    }

    boost::asio::ip::tcp::socket& socket() { return socket_; }

    // call once connection accepted/established
    void start() {
        configure_socket();
        // begin receive chain
        boost::asio::post(strand_, [self = shared_from_this()]() {
            self->do_receive();
        });
    }

    // send API (keeps name compatibility with original code which used send_impl internally)
    // This is a lightweight wrapper that copies the provided bytes into either the send ring (fast path)
    // or into the fallback chunk deque (no monolithic vector reallocations).
    void send_impl(std::span<const uint8_t> pdu_bytes)
    {
        // called from strand / or posted to strand by caller in original design
        boost::asio::post(strand_, [self = shared_from_this(), vec = std::vector<uint8_t>(pdu_bytes.begin(), pdu_bytes.end())]() mutable {
            self->enqueue_or_fallback(std::move(vec));
        });
    }

    // same name as original: allow external code to set threshold
    void set_send_buf_threshold(size_t s) { send_buf_threshold_ = s; }

    bool is_send_buf_above_threshold() const {
        return (send_ring_.space_remaining() < send_buf_threshold_) || (!pending_send_chunks_.empty());
    }

    bool can_send_without_blocking() const {
        return send_ring_.space_remaining() > send_buf_threshold_ && pending_send_chunks_.empty();
    }

    void pause_receiving()
    {
        if (receiving_state_ == receiving_state::receiving)
            receiving_state_ = receiving_state::pending_pause;
    }

    void resume_receiving()
    {
        auto prev = std::exchange(receiving_state_, receiving_state::receiving);
        if (prev == receiving_state::paused)
            boost::asio::post(strand_, [self = shared_from_this()]() { self->do_receive(); });
    }

    // Close with reason (preserved signature used in your original header)
    void close(const std::string& reason)
    {
        boost::asio::post(strand_, [w = weak_from_this(), reason]() {
            if (auto self = w.lock()) {
                if (self->state_ == state::close) return;
                self->pause_receiving();
                std::optional<std::string> err;
                if (self->state_ == state::open) err = reason;
                self->state_ = state::close;
                boost::system::error_code ec;
                self->socket_.close(ec);
                if (self->close_handler) self->close_handler(self->shared_from_this(), err);
            }
        });
    }

  private:
    // configure socket options: TCP_NODELAY, buffer sizes, keepalive
    void configure_socket()
    {
        boost::system::error_code ec;
        if (options_.no_delay) {
            socket_.set_option(boost::asio::ip::tcp::no_delay(true), ec);
            (void)ec;
        }
        if (options_.sndbuf > 0) {
            socket_.set_option(boost::asio::socket_base::send_buffer_size(options_.sndbuf), ec);
            (void)ec;
        }
        if (options_.rcvbuf > 0) {
            socket_.set_option(boost::asio::socket_base::receive_buffer_size(options_.rcvbuf), ec);
            (void)ec;
        }
        if (options_.keepalive) {
            enable_keepalive(socket_, /*inactivity_timeout=*/0);
        }
    }

    // enqueue bytes into send_ring if possible; otherwise push remainder into pending_send_chunks_
    void enqueue_or_fallback(std::vector<uint8_t> bytes)
    {
        // try to copy into send ring (fast path) in a loop
        size_t written = 0;
        while (written < bytes.size()) {
            auto [ptr, len] = send_ring_.prepare_write();
            if (len == 0) break; // no space
            size_t part = std::min(len, bytes.size() - written);
            std::memcpy(ptr, bytes.data() + written, part);
            send_ring_.produced(part);
            written += part;
        }

        if (written < bytes.size()) {
            // remaining -> fallback chunk (avoid large single vector growth)
            std::vector<uint8_t> rem(bytes.begin() + written, bytes.end());
            pending_send_chunks_.emplace_back(std::move(rem));
        }

        // if nothing is currently sending, start
        if (!send_in_progress_) {
            start_send_chain();
        }
    }

    // Build a small array of const_buffers (avoid per-call vector allocations)
    static constexpr size_t max_buffer_segments = 8;

    void start_send_chain()
    {
        if (send_in_progress_) return;
        // we run inside strand
        send_in_progress_ = true;
        post_send_work();
    }

    void post_send_work()
    {
        // prepare small array of buffers from send_ring_ and pending_send_chunks_
        std::array<boost::asio::const_buffer, max_buffer_segments> bufs;
        size_t buf_count = 0;
        size_t total_bytes = 0;

        // gather from send_ring_ first (zero-copy)
        if (send_ring_.size() > 0) {
            auto spans = send_ring_.peek_two_spans(options_.max_send_chunk);
            if (!spans.first.empty()) {
                bufs[buf_count++] = boost::asio::buffer(spans.first.data(), spans.first.size());
                total_bytes += spans.first.size();
            }
            if (!spans.second.empty() && total_bytes < options_.max_send_chunk) {
                size_t take = std::min(spans.second.size(), options_.max_send_chunk - total_bytes);
                bufs[buf_count++] = boost::asio::buffer(spans.second.data(), take);
                total_bytes += take;
            }
        }

        // append pending fallback chunks (each chunk is a vector)
        while (!pending_send_chunks_.empty() && total_bytes < options_.max_send_chunk && buf_count < max_buffer_segments) {
            auto &front = pending_send_chunks_.front();
            size_t take = std::min(front.size(), options_.max_send_chunk - total_bytes);
            bufs[buf_count++] = boost::asio::buffer(front.data(), take);
            total_bytes += take;
            if (take < front.size()) {
                // consume partial: move remainder to front of deque efficiently
                std::vector<uint8_t> remainder(front.begin() + take, front.end());
                front.swap(remainder);
                break;
            } else {
                pending_send_chunks_.pop_front();
            }
        }

        if (buf_count == 0) {
            send_in_progress_ = false;
            // notify availability if we just freed space
            if (send_buf_available_handler) send_buf_available_handler();
            return;
        }

        auto self = shared_from_this();
        boost::asio::async_write(socket_,
            boost::asio::buffer(bufs.data(), buf_count),
            boost::asio::bind_executor(strand_,
                [self, total_bytes](const boost::system::error_code &ec, std::size_t bytes_transferred) {
                    self->on_send_complete(ec, bytes_transferred);
                }));
    }

    void on_send_complete(const boost::system::error_code& ec, std::size_t bytes_transferred)
    {
        if (ec) {
            // error path: notify and close
            if (deserialization_error_handler) {
                // not the usual use but we keep error callback available
            }
            if (close_handler) close_handler(shared_from_this(), std::optional<std::string>(ec.message()));
            close(ec.message());
            send_in_progress_ = false;
            return;
        }

        // consume from send_ring_ first
        size_t remaining = bytes_transferred;
        if (send_ring_.size() > 0) {
            size_t avail = send_ring_.size();
            size_t take = std::min(avail, remaining);
            if (take) {
                send_ring_.consume(take);
                remaining -= take;
            }
        }

        // consume from pending chunks
        while (remaining && !pending_send_chunks_.empty()) {
            auto &front = pending_send_chunks_.front();
            if (remaining >= front.size()) {
                remaining -= front.size();
                pending_send_chunks_.pop_front();
            } else {
                // partial consume: remove the first 'remaining' bytes
                std::vector<uint8_t> rem(front.begin() + remaining, front.end());
                front.swap(rem);
                remaining = 0;
            }
        }

        stats_send_bytes_.fetch_add(bytes_transferred, std::memory_order_relaxed);

        // If more data remains in buffers, chain another send immediately
        if (send_ring_.size() > 0 || !pending_send_chunks_.empty()) {
            post_send_work();
        } else {
            // no pending data
            send_in_progress_ = false;
            if (send_buf_available_handler) send_buf_available_handler();
        }
    }

    // Receive path (zero-copy friendly)
    void do_receive()
    {
        if (state_ == state::close) return;
        if (receiving_state_ == receiving_state::paused) return;

        // prepare write location in recv_ring_
        auto [ptr, len] = recv_ring_.prepare_write();
        if (len == 0) {
            // ring full -> temporarily pause and retry shortly
            receiving_state_ = receiving_state::paused;
            // schedule resume after small backoff to allow consumer to process
            auto timer = std::make_shared<boost::asio::steady_timer>(socket_.get_executor().context());
            timer->expires_after(std::chrono::milliseconds(5));
            auto self = shared_from_this();
            timer->async_wait(boost::asio::bind_executor(strand_, [self, timer](const boost::system::error_code& ec) {
                if (!ec && self->state_ != state::close) {
                    self->receiving_state_ = receiving_state::receiving;
                    self->do_receive();
                }
            }));
            return;
        }

        // async_read_some directly into ring writable area
        auto self = shared_from_this();
        socket_.async_read_some(boost::asio::buffer(ptr, len),
            boost::asio::bind_executor(strand_,
                [self](const boost::system::error_code& ec, std::size_t bytes_read) {
                    if (ec) {
                        if (self->close_handler) self->close_handler(self, std::optional<std::string>(ec.message()));
                        self->close(ec.message());
                        return;
                    }
                    if (bytes_read == 0) {
                        // connection closed cleanly
                        self->close("peer closed");
                        return;
                    }
                    self->recv_ring_.produced(bytes_read);
                    self->stats_recv_bytes_.fetch_add(bytes_read, std::memory_order_relaxed);

                    // process PDUs available in the ring (zero-copy when possible)
                    self->process_recv_ring();

                    // continue receiving
                    if (self->receiving_state_ == receiving_state::receiving && self->state_ != state::close) {
                        self->do_receive();
                    } else if (self->receiving_state_ == receiving_state::pending_pause) {
                        self->receiving_state_ = receiving_state::paused;
                    }
                }));
    }

    // parse loop: uses 4-byte length prefix interpretation like original (adjust if your protocol differs)
    void process_recv_ring()
    {
        for (;;) {
            size_t available = recv_ring_.size();
            if (available < 4) return; // not enough for length header

            // get first 4 bytes (might be in two spans)
            auto spans_len = recv_ring_.peek_two_spans(4);
            uint8_t lenb[4];
            if (!spans_len.first.empty() && spans_len.first.size() == 4) {
                std::memcpy(lenb, spans_len.first.data(), 4);
            } else {
                // copy split header into small stack buffer (cheap)
                size_t copied = 0;
                if (!spans_len.first.empty()) {
                    std::memcpy(lenb, spans_len.first.data(), spans_len.first.size());
                    copied = spans_len.first.size();
                }
                if (!spans_len.second.empty()) {
                    std::memcpy(lenb + copied, spans_len.second.data(), 4 - copied);
                }
            }
            uint32_t command_length = (uint32_t(lenb[0]) << 24) | (uint32_t(lenb[1]) << 16) |
                                      (uint32_t(lenb[2]) << 8) | uint32_t(lenb[3]);

            if (command_length < header_length) {
                close("Invalid command_length");
                return;
            }
            if (command_length > max_command_length) {
                close("Command too large");
                return;
            }
            if (available < command_length) return; // wait for full pdu

            // Now fetch two spans covering full pdu (zero-copy)
            auto spans = recv_ring_.peek_two_spans(command_length);

            // attempt to deserialize directly from spans to avoid copies;
            // This requires you to implement a parser that can accept either:
            // - contiguous span (spans.second empty) OR
            // - two spans (spans.first, spans.second) representing head and wrap tail
            // Here we call the same handlers as original: request_handler/response_handler
            // You must adapt the below deserialization to your PDU types (request/response).
            try {
                // Minimal example: extract command_id from header (assume header layout known)
                // NOTE: Replace this block with your actual pdu deserialization using spans
                // For demonstration, we call a helper that must exist in your project:
                // auto pdu = pdu_from_spans(spans.first, spans.second);
                // and then call request_handler or response_handler accordingly.
                // For compatibility, we call a simplified path: copy into body_reuse_buf_ only if necessary.

                if (!spans.second.empty()) {
                    // multi-span: create minimal contiguous buffer only when necessary for your parser
                    body_reuse_buf_.clear();
                    body_reuse_buf_.insert(body_reuse_buf_.end(), spans.first.begin(), spans.first.end());
                    body_reuse_buf_.insert(body_reuse_buf_.end(), spans.second.begin(), spans.second.end());
                    // Now feed body_reuse_buf_ to your parser (placeholder)
                    // Example (pseudo):
                    // auto pdu = parse_request_from_bytes(body_reuse_buf_);
                    // dispatch...
                    // For now, just consume
                } else {
                    // contiguous span available - prefer zero-copy parse
                    // Example (pseudo):
                    // auto pdu = parse_request_from_span(spans.first);
                    // dispatch...
                }
            } catch (const std::exception &ex) {
                if (deserialization_error_handler) {
                    // user expects signature (const std::string&, command_id, std::span<const uint8_t>)
                    // we can't extract command_id generically here; pass a placeholder or adapt accordingly
                    deserialization_error_handler(ex.what(), command_id{0}, std::span<const uint8_t>()); // replace with actual command id and data
                }
                close("Deserialization error");
                return;
            }

            // consume the pdu bytes
            recv_ring_.consume(command_length);
        }
    }

  private:
    // low-level buffers and structures
    boost::asio::ip::tcp::socket socket_;
    boost::asio::strand<boost::asio::io_context::executor_type> strand_;
    Options options_;

    // Use your project's ring buffer type for actual operations (kept name compatibility)
    detail::ring_buffer<uint8_t, default_ring_capacity> send_ring_;
    detail::ring_buffer<uint8_t, default_ring_capacity> recv_ring_;

    // Fallback pending send chunks (avoid single monolithic vector)
    std::deque<std::vector<uint8_t>> pending_send_chunks_;

    // If you still need a writing_send_buf_ for special code paths, keep it but don't shrink_to_fit on hot path
    std::vector<uint8_t> writing_send_buf_;

    // small reusable buffer for parser when parser cannot accept two spans
    std::vector<uint8_t> body_reuse_buf_;

    // session state
    state state_;
    receiving_state receiving_state_;

    // send flow control
    bool send_in_progress_;
    size_t send_buf_threshold_;

    // handlers: already declared public members in original header, so not repeated here

    // statistics
    std::atomic<uint64_t> stats_send_bytes_;
    std::atomic<uint64_t> stats_recv_bytes_;
}; // class session

} // namespace pa::pinex
